{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6f96a3",
   "metadata": {},
   "source": [
    "<font size=\"+3\"><strong>8.3. Predicting Volatility</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dc22d",
   "metadata": {},
   "source": [
    "In the last lesson, we learned that one characteristic of stocks that's important to investors is **volatility**. Actually, it's so important that there are several time series models for predicting it. In this lesson, we'll build one such model called **GARCH**. We'll also continue working with assert statements to test our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "from config import settings\n",
    "from data import SQLRepository\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc9473",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "As always, the first thing we need to do is connect to our data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16ba98",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeaa7f",
   "metadata": {},
   "source": [
    "**Task 8.3.1:** Create a connection to your database and then instantiate a `SQLRepository` named `repo` to interact with that database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40279974",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(settings.db_name, check_same_thread=False)\n",
    "repo = SQLRepository(connection=connection)\n",
    "\n",
    "print(\"repo type:\", type(repo))\n",
    "print(\"repo.connection type:\", type(repo.connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6585d",
   "metadata": {},
   "source": [
    "Now that we're connected to a database, let's pull out what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9863c",
   "metadata": {},
   "source": [
    "**Task 8.3.2:** Pull the most recent 2,500 rows of data for Ambuja Cement from your database. Assign the results to the variable `df_ambuja`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ambuja = repo.read_table(table_name=\"AMBUJACEM.BSE\", limit=2500)\n",
    "\n",
    "print(\"df_ambuja type:\", type(df_ambuja))\n",
    "print(\"df_ambuja shape:\", df_ambuja.shape)\n",
    "df_ambuja.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7564768",
   "metadata": {},
   "source": [
    "To train our model, the only data we need are the daily returns for `\"AMBUJACEM.BSE\"`. We learned how to calculate returns in the last lesson, but now let's formalize that process with a wrangle function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd84fe",
   "metadata": {},
   "source": [
    "**Task 8.3.3:** Create a `wrangle_data` function whose output is the returns for a stock stored in your database. Use the docstring as a guide and the assert statements in the following code block to test your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa133f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data(ticker, n_observations):\n",
    "\n",
    "    \"\"\"Extract table data from database. Calculate returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker : str\n",
    "        The ticker symbol of the stock (also table name in database).\n",
    "\n",
    "    n_observations : int\n",
    "        Number of observations to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Name will be `\"return\"`. There will be no `NaN` values.\n",
    "    \"\"\"\n",
    "    # Get table from database\n",
    "    df = repo.read_table(table_name=ticker, limit=n_observations)\n",
    "\n",
    "    # Sort DataFrame ascending by date\n",
    "    df.sort_index(ascending = True, inplace=True)\n",
    "\n",
    "    # Create \"return\" column\n",
    "    df[\"return\"] = df[\"close\"].pct_change() * 100\n",
    "    \n",
    "    # Return returns\n",
    "    return df[\"return\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c91bf7",
   "metadata": {},
   "source": [
    "When you run the cell below to test your function, you'll also create a Series `y_ambuja` that we'll use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ambuja = wrangle_data(ticker=\"AMBUJACEM.BSE\", n_observations=2500)\n",
    "\n",
    "# Is `y_ambuja` a Series?\n",
    "assert isinstance(y_ambuja, pd.Series)\n",
    "\n",
    "# Are there 2500 observations in the Series?\n",
    "assert len(y_ambuja) == 99\n",
    "\n",
    "# Is `y_ambuja` name \"return\"?\n",
    "assert y_ambuja.name == \"return\"\n",
    "\n",
    "# Does `y_ambuja` have a DatetimeIndex?\n",
    "assert isinstance(y_ambuja.index, pd.DatetimeIndex)\n",
    "\n",
    "# Is index sorted ascending?\n",
    "assert all(y_ambuja.index == y_ambuja.sort_index(ascending=True).index)\n",
    "\n",
    "# Are there no `NaN` values?\n",
    "assert y_ambuja.isnull().sum() == 0\n",
    "\n",
    "y_ambuja.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f0799",
   "metadata": {},
   "source": [
    "Great work! Now that we've got a wrangle function, let's get the returns for Suzlon Energy, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf786a",
   "metadata": {},
   "source": [
    "**Task 8.3.4:** Use your `wrangle_data` function to get the returns for the 2,500 most recent trading days of Suzlon Energy. Assign the results to `y_suzlon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faeca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_suzlon = wrangle_data(ticker=\"SUZLON.BSE\", n_observations=2500)\n",
    "\n",
    "print(\"y_suzlon type:\", type(y_suzlon))\n",
    "print(\"y_suzlon shape:\", y_suzlon.shape)\n",
    "y_suzlon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850daff3",
   "metadata": {},
   "source": [
    "# Explore\n",
    "Let's recreate the volatility time series plot we made in the last lesson so that we have a visual aid to talk about what volatility is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot returns for `df_suzlon` and `df_ambuja`\n",
    "y_suzlon.plot(ax=ax, label=\"SUZLON\")\n",
    "y_ambuja.plot(ax=ax, label=\"AMBUJACEM\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4566f",
   "metadata": {},
   "source": [
    "The above plot shows how returns change over time. This may seem like a totally new concept, but if we visualize them without considering time, things will start to look familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947412bd",
   "metadata": {},
   "source": [
    "**Task 8.3.5:** Create a histogram `y_ambuja` with 25 bins. Be sure to label the x-axis `\"Returns\"`, the y-axis `\"Frequency [count]\"`, and use the title `\"Distribution of Ambuja Cement Daily Returns\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of `y_ambuja`, 25 bins\n",
    "y_ambuja.hist(bins=25)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Ambuja Cement Daily Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee38d1",
   "metadata": {},
   "source": [
    "This is a familiar shape! It turns out that returns follow an almost normal distribution, centered on `0`. **Volatility** is the measure of the spread of these returns around the mean. In other words, volatility in finance is the same thing at standard deviation in statistics.\n",
    "\n",
    "Let's start by measuring the daily volatility of our two stocks. Since our data frequency is also daily, this will be exactly the same as calculating the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0242d",
   "metadata": {},
   "source": [
    "**Task 8.3.6:** Calculate daily volatility for Suzlon and Ambuja, assigning them to the variables `suzlon_daily_volatility` and `ambuja_daily_volatility`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suzlon_daily_volatility = y_suzlon.std()\n",
    "ambuja_daily_volatility = y_ambuja.std()\n",
    "\n",
    "print(\"Suzlon Daily Volatility:\", suzlon_daily_volatility)\n",
    "print(\"Ambuja Daily Volatility:\", ambuja_daily_volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdebdd0",
   "metadata": {},
   "source": [
    "Looks like Suzlon is more volatile than Ambuja. This reinforces what we saw in our time series plot, where Suzlon returns have a much wider spread.\n",
    "\n",
    "While daily volatility is useful, investors are also interested in volatility over other time periods â€” like annual volatility. Keep in mind that a year isn't 365 days for a stock market, though. After excluding weekends and holidays, most markets have only 252 trading days.\n",
    "\n",
    "So how do we go from daily to annual volatility? The same way we calculated the standard deviation for our multi-day experiment in Project 7!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129b063",
   "metadata": {},
   "source": [
    "**Task 8.3.7:** Calculate the annual volatility for Suzlon and Ambuja, assigning the results to `suzlon_annual_volatility` and `ambuja_annual_volatility`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "suzlon_annual_volatility = suzlon_daily_volatility * np.sqrt(252)\n",
    "ambuja_annual_volatility = ambuja_daily_volatility * np.sqrt(252)\n",
    "\n",
    "print(\"Suzlon Annual Volatility:\", suzlon_annual_volatility)\n",
    "print(\"Ambuja Annual Volatility:\", ambuja_annual_volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e7203",
   "metadata": {},
   "source": [
    "Again, Suzlon has higher volatility than Ambuja. What do you think it means that the annual volatility is larger than daily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122a4c7",
   "metadata": {},
   "source": [
    "Since we're dealing with time series data, another way to look at volatility is by calculating it using a rolling window. We'll do this the same way we calculated the rolling average for PM 2.5 levels in Project 3. Here, we'll start focusing on Ambuja Cement exclusively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7bc11",
   "metadata": {},
   "source": [
    "**Task 8.3.8:** Calculate the rolling volatility for `y_ambuja`, using a 50-day window. Assign the result to `ambuja_rolling_50d_volatility`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambuja_rolling_50d_volatility = y_ambuja.rolling(window=50).std().dropna()\n",
    "\n",
    "print(\"rolling_50d_volatility type:\", type(ambuja_rolling_50d_volatility))\n",
    "print(\"rolling_50d_volatility shape:\", ambuja_rolling_50d_volatility.shape)\n",
    "ambuja_rolling_50d_volatility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a97d1",
   "metadata": {},
   "source": [
    "This time, we'll focus on Ambuja Cement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f77a38",
   "metadata": {},
   "source": [
    "**Task 8.3.9:** Create a time series plot showing the daily returns for Ambuja Cement and the 50-day rolling volatility. Be sure to label your axes and include a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot `y_ambuja`\n",
    "y_ambuja.plot(ax=ax, label=\"daily return\")\n",
    "\n",
    "# Plot `ambuja_rolling_50d_volatility`\n",
    "ambuja_rolling_50d_volatility.plot(ax=ax, label=\"50-day rolling volatility.\",linewidth = 3)\n",
    "\n",
    "# Add x-axis label\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db41c59",
   "metadata": {},
   "source": [
    "Here we can see that volatility goes up when the returns change drastically â€” either up or down. For instance, we can see a big increase in volatility in May 2020, when there were several days of large negative returns. We can also see volatility go down in August 2022, when there are only small day-to-day changes in returns.\n",
    "\n",
    "This plot reveals a problem. We want to use returns to see if high volatility on one day is associated with high volatility on the following day. But high volatility is caused by large changes in returns, which can be either positive or negative. How can we assess negative and positive numbers together without them canceling each other out? One solution is to take the absolute value of the numbers, which is what we do to calculate performance metrics like mean absolute error. The other solution, which is more common in this context, is to square all the values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8defdf",
   "metadata": {},
   "source": [
    "**Task 8.3.10:** Create a time series plot of the squared returns in `y_ambuja`. Don't forget to label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8eae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot squared returns\n",
    "(y_ambuja**2).plot(ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Squared Return\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87912a6b",
   "metadata": {},
   "source": [
    "Perfect! Now it's much easier to see that (1) we have periods of high and low volatility, and (2) high volatility days tend to cluster together. This is a perfect situation to use a GARCH model.\n",
    "\n",
    "A GARCH model is sort of like the ARMA model we learned about in Lesson 3.4. It has a `p` parameter handling correlations at prior time steps and a `q` parameter for dealing with \"shock\" events. It also uses the notion of lag. To see how many lags we should have in our model, we should create an ACF and PACF plot â€” but using the squared returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b57ac",
   "metadata": {},
   "source": [
    "**Task 8.3.11:** Create an ACF plot of squared returns for Ambuja Cement. Be sure to label your x-axis `\"Lag [days]\"` and your y-axis `\"Correlation Coefficient\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be680b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared returns\n",
    "plot_acf(y_ambuja**2,ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a21cc",
   "metadata": {},
   "source": [
    "**Task 8.3.12:** Create a PACF plot of squared returns for Ambuja Cement. Be sure to label your x-axis `\"Lag [days]\"` and your y-axis `\"Correlation Coefficient\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared returns\n",
    "plot_pacf(y_ambuja**2,ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9822a8",
   "metadata": {},
   "source": [
    "In our PACF, it looks like a lag of 3 would be a good starting point. \n",
    "\n",
    "Normally, at this point in the model building process, we would split our data into training and test sets, and then set a baseline. Not this time. This is because our model's input and its output are two different measurements. We'll use **returns** to train our model, but we want it to predict **volatility**. If we created a test set, it wouldn't give us the \"true values\" that we'd need to assess our model's performance. So this time, we'll skip right to iterating. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4108351",
   "metadata": {},
   "source": [
    "# Split\n",
    "The last thing we need to do before building our model is to create a training set. Note that we won't create a test set here. Rather, we'll use all of y_ambuja to conduct walk-forward validation after we've built our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15597d7b",
   "metadata": {},
   "source": [
    "**Task 8.3.13:** Create a training set `y_ambuja_train` that contains the first 80% of the observations in `y_ambuja`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test = int(len(y_ambuja) * 0.8)\n",
    "y_ambuja_train = y_ambuja.iloc[:cutoff_test]\n",
    "\n",
    "print(\"y_ambuja_train type:\", type(y_ambuja_train))\n",
    "print(\"y_ambuja_train shape:\", y_ambuja_train.shape)\n",
    "y_ambuja_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db81498",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "Just like we did the last time we built a model like this, we'll begin by iterating.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "\n",
    "# Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9d41f",
   "metadata": {},
   "source": [
    "**Task 8.3.14:** Build and fit a GARCH model using the data in `y_ambuja`. Start with `3` as the value for `p` and `q`. Then use the model summary to assess its performance and try other lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8979d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = arch_model(\n",
    "    y_ambuja_train,\n",
    "    p=1,\n",
    "    q=1,\n",
    "    rescale = False\n",
    ").fit(disp=0)\n",
    "print(\"model type:\", type(model))\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430655ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Tip:</b> You access the AIC and BIC scores programmatically. Every <code>ARCHModelResult</code> has an <code>.aic</code> and a <code>.bic</code> attribute. Try it for yourself: enter <code>model.aic</code> or <code>model.bic</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c552437a",
   "metadata": {},
   "source": [
    "Now that we've settled on a model, let's visualize its predictions, together with the Ambuja returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263afaba",
   "metadata": {},
   "source": [
    "**Task 8.3.15:** Create a time series plot with the Ambuja returns and the conditional volatility for your `model`. Be sure to include axis labels and add a legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot `y_ambuja_train`\n",
    "y_ambuja_train.plot(ax=ax, label = \"Ambuja Daily Return\")\n",
    "\n",
    "# Plot conditional volatility * 2\n",
    "(2*model.conditional_volatility).plot(\n",
    "    ax=ax, color=\"orange\", label=\"Conditional Volatility\", linewidth=3\n",
    ")\n",
    "\n",
    "# Plot conditional volatility * -2\n",
    "(-2*model.conditional_volatility).plot(\n",
    "    ax=ax, color=\"orange\", linewidth=3\n",
    ")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "\n",
    "\n",
    "# Add legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe874f",
   "metadata": {},
   "source": [
    "Visually, our model looks pretty good, but we should examine residuals, just to make sure. In the case of GARCH models, we need to look at the standardized residuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3fcb9",
   "metadata": {},
   "source": [
    "**Task 8.3.16:** Create a time series plot of the standardized residuals for your `model`. Be sure to include axis labels and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53df99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot standardized residuals\n",
    "model.std_resid.plot(ax=ax, label = \"Standardized Residuals\")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "\n",
    "# Add legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6e68a",
   "metadata": {},
   "source": [
    "These residuals look good: they have a consistent mean and spread over time. Let's check their normality using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d757c69",
   "metadata": {},
   "source": [
    "**Task 8.3.17:** Create a histogram with 25 bins of the standardized residuals for your model. Be sure to label your axes and use a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of standardized residuals, 25 bins\n",
    "plt.hist(model.std_resid, bins=25)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Standardized Residual\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Standardized Residual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713d3cd",
   "metadata": {},
   "source": [
    "Our last visualization will the ACF of standardized residuals. Just like we did with our first ACF, we'll need to square the values here, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465f686",
   "metadata": {},
   "source": [
    "**Task 8.3.18:** Create an ACF plot of the square of your standardized residuals. Don't forget axis labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared, standardized residuals\n",
    "plot_acf(model.std_resid**2,ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85208bbf",
   "metadata": {},
   "source": [
    "Excellent! Looks like this model is ready for a final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614638a0",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "To evaluate our model, we'll do walk-forward validation. Before we do, let's take a look at how this model returns its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37423743",
   "metadata": {},
   "source": [
    "**Task 8.3.19:** Create a one-day forecast from your `model` and assign the result to the variable `one_day_forecast`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_forecast = model.forecast(horizon=1, reindex=False).variance\n",
    "\n",
    "print(\"one_day_forecast type:\", type(one_day_forecast))\n",
    "one_day_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3370a",
   "metadata": {},
   "source": [
    "There are two things we need to keep in mind here. First, our `model` forecast shows the predicted **variance**, not the **standard deviation** / **volatility**. So we'll need to take the square root of the value. Second, the prediction is in the form of a DataFrame. It has a DatetimeIndex, and the date is the last day for which we have training data. The `\"h.1\"` column stands for \"horizon 1\", that is, our model's prediction for the following day. We'll have to keep all this in mind when we reformat this prediction to serve to the end user of our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0b737",
   "metadata": {},
   "source": [
    "**Task 8.3.20:** Complete the code below to do walk-forward validation on your `model`. Then run the following code block to visualize the model's test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to hold predictions\n",
    "predictions = []\n",
    "\n",
    "# Calculate size of test data (20%)\n",
    "test_size = int(len(y_ambuja) * 0.2)\n",
    "\n",
    "# Walk forward\n",
    "for i in range(test_size):\n",
    "    # Create test data\n",
    "    y_train = y_ambuja.iloc[: -(test_size - i)]\n",
    "\n",
    "    # Train model\n",
    "    model = model = arch_model(y_train,p=1,q=1,rescale = False).fit(disp=0)\n",
    "\n",
    "    # Generate next prediction (volatility, not variance)\n",
    "    next_pred = model.forecast(horizon=1, reindex=False).variance.iloc[0,0]**0.5\n",
    "\n",
    "    # Append prediction to list\n",
    "    predictions.append(next_pred)\n",
    "\n",
    "# Create Series from predictions list\n",
    "y_test_wfv = pd.Series(predictions, index=y_ambuja.tail(test_size).index)\n",
    "\n",
    "print(\"y_test_wfv type:\", type(y_test_wfv))\n",
    "print(\"y_test_wfv shape:\", y_test_wfv.shape)\n",
    "y_test_wfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot returns for test data\n",
    "y_ambuja.tail(test_size).plot(ax=ax, label=\"Ambuja Return\")\n",
    "\n",
    "# Plot volatility predictions * 2\n",
    "(2 * y_test_wfv).plot(ax=ax, c=\"C1\", label=\"2 SD Predicted Volatility\")\n",
    "\n",
    "# Plot volatility predictions * -2\n",
    "(-2 * y_test_wfv).plot(ax=ax, c=\"C1\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2cd96",
   "metadata": {},
   "source": [
    "This looks pretty good. Our volatility predictions seem to follow the changes in returns over time. This is especially clear in the low-volatility period in the summer of 2022 and the high-volatility period in fall 2022.\n",
    "\n",
    "One additional step we could do to evaluate how our `model` performs on the test data would be to plot the ACF of the standardized residuals for only the test set. But you can do that step on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06dc097",
   "metadata": {},
   "source": [
    "# Communicate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c9685",
   "metadata": {},
   "source": [
    "Normally in this section, we create visualizations for a human audience, but our goal for *this* project is to create an API for a *computer* audience. So we'll focus on transforming our model's predictions to JSON format, which is what we'll use to send predictions in our application. \n",
    "\n",
    "The first thing we need to do is create a DatetimeIndex for our predictions. Using labels like `\"h.1\"`, `\"h.2\"`, etc., won't work. But there are two things we need to keep in mind. First, we can't include dates that are weekends because no trading happens on those days. And we'll need to write our dates using strings that follow the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) standard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3f705",
   "metadata": {},
   "source": [
    "**Task 8.3.21:** Below is a `prediction`, which contains a 5-day forecast from our `model`. Using it as a starting point, create a `prediction_index`. This should be a list with the following 5 dates written in ISO 8601 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5-day volatility forecast\n",
    "prediction = model.forecast(horizon=5, reindex=False).variance ** 0.5\n",
    "print(prediction)\n",
    "\n",
    "# Calculate forecast start date\n",
    "start = prediction.index[0] + pd.DateOffset(days=1)\n",
    "\n",
    "# Create date range\n",
    "prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "# Create prediction index labels, ISO 8601 format\n",
    "prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "print(\"prediction_index type:\", type(prediction_index))\n",
    "print(\"prediction_index len:\", len(prediction_index))\n",
    "prediction_index[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d549d5",
   "metadata": {},
   "source": [
    "Now that we know how to create the index, let's create a function to combine the index and predictions, and then return a dictionary where each key is a date and each value is a predicted volatility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd83c49",
   "metadata": {},
   "source": [
    "**Task 8.3.22:** Create a `clean_prediction` function. It should take a variance prediction DataFrame as input and return a dictionary where each key is a date in ISO 8601 format and each value is the predicted volatility. Use the docstring as a guide and the assert statements to test your function. When you're satisfied with the result, submit it to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7839b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(prediction):\n",
    "\n",
    "    \"\"\"Reformat model prediction to JSON.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : pd.DataFrame\n",
    "        Variance from a `ARCHModelForecast`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Forecast of volatility. Each key is date in ISO 8601 format.\n",
    "        Each value is predicted volatility.\n",
    "    \"\"\"\n",
    "    # Calculate forecast start date\n",
    "    start = prediction.index[0] + pd.DateOffset(days=1)\n",
    "\n",
    "    # Create date range\n",
    "    prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "    # Create prediction index labels, ISO 8601 format\n",
    "    prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "\n",
    "    # Extract predictions from DataFrame, get square root\n",
    "    data = prediction.values.flatten()**0.5\n",
    "\n",
    "    # Combine `data` and `prediction_index` into Series\n",
    "    prediction_formatted = pd.Series(data,index = prediction_index)\n",
    "\n",
    "    # Return Series as dictionary\n",
    "    return prediction_formatted.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7baa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.forecast(horizon=10, reindex=False).variance\n",
    "prediction_formatted = clean_prediction(prediction)\n",
    "\n",
    "# Is `prediction_formatted` a dictionary?\n",
    "assert isinstance(prediction_formatted, dict)\n",
    "\n",
    "# Are keys correct data type?\n",
    "assert all(isinstance(k, str) for k in prediction_formatted.keys())\n",
    "\n",
    "# Are values correct data type\n",
    "assert all(isinstance(v, float) for v in prediction_formatted.values())\n",
    "\n",
    "prediction_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e67c2",
   "metadata": {},
   "source": [
    "Great work! We now have several components for our application: classes for getting data from an API, classes for storing it in a database, and code for building our model and cleaning our predictions. The next step is creating a class for our model and paths for application â€” both of which we'll do in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa733c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
